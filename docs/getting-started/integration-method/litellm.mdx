---
title: "LiteLLM"
og:title: "LiteLLM Integration with Helicone"
og:description: "Open source observability for LiteLLM. Automatically capture detailed traces and metrics for every request using the OpenAI format, including Bedrock, Azure, Cohere, Anthropic, Ollama, Sagemaker, HuggingFace, and Replicate (100+ LLMs)"
description: "Open source observability for LiteLLM. Capture detailed traces and metrics for every request using OpenAI or GPT format."
---

[LiteLLM](https://github.com/BerriAI/litellm) is a model I/O library that standardizes API calls to various LLM providers like Bedrock, Azure, Cohere, Anthropic, Ollama, Sagemaker, HuggingFace, and Replicate (100+ LLMs).

There are 2 ways to integrate LiteLLM with Helicone:

1. **Use Callbacks**: Log your API calls directly to Helicone using LiteLLM callbacks. This method captures detailed traces and metrics with minimal code changes.

2. **Use Helicone as a Proxy**: Route your API calls through Helicone to leverage advanced features like caching and request management. Ideal for optimizing OpenAI and Azure integrations.

# Approach 1: Use Callbacks

<Check>1 line integration</Check>

Add `HELICONE_API_KEY` to your environment variables.

```bash
export HELICONE_API_KEY=sk-<your-api-key>
# You can also set it in your code (See below)
```

Tell LiteLLM you want to log your data to Helicone

```python
litellm.success_callback=["helicone"]
```

## Complete code

```python
from litellm import completion

## set env variables
os.environ["HELICONE_API_KEY"] = "your-helicone-key"
os.environ["OPENAI_API_KEY"], os.environ["COHERE_API_KEY"] = "", ""

# set callbacks
litellm.success_callback=["helicone"]

#openai call
response = completion(model="gpt-3.5-turbo", messages=[{"role": "user", "content": "Hi ðŸ‘‹ - i'm openai"}])

#cohere call
response = completion(model="command-nightly", messages=[{"role": "user", "content": "Hi ðŸ‘‹ - i'm cohere"}])

```

# Approach 2: [OpenAI + Azure only] Use Helicone as a proxy

Helicone provides advanced functionality like caching, rate limiting, prompt threat detection, OpenAI moderations, and more, specifically optimized for Azure and OpenAI integrations.

If you want to use Helicone to proxy your OpenAI/Azure requests, then you can -

<Check>2 line integration</Check>

```python
litellm.api_url = ""https://oai.helicone.ai/v1"" # set the base url
litellm.headers = {"Helicone-Auth": f"Bearer {os.getenv('HELICONE_API_KEY')}"} # set your headers
```

## Complete code

```python
import litellm
from litellm import completion

litellm.api_base = "https://oai.helicone.ai/v1"
litellm.headers = {"Helicone-Auth": f"Bearer {os.getenv('HELICONE_API_KEY')}"}

response = litellm.completion(
    model="gpt-3.5-turbo",
    messages=[{"role": "user", "content": "how does a court case get to the Supreme Court?"}]
)

print(response)
```

Feel free to [check it out](https://github.com/BerriAI/litellm) and tell us what you think ðŸ‘‹
